# NNUE 2

## Summary

This document contains information about Pingu's NNUE. It is the sequel to [nnue.md](/docs/nnue.md) and it describes the process of training the NNUE for [Pingu 4.0.0](https://github.com/WillChing01/Pingu/releases/tag/v4.0.0).

Only the model weights were changed when going from Pingu 3.0.0 to Pingu 4.0.0, so the prior information about [model architecture](/docs/nnue.md#model-architecture) and [implementation](/docs/nnue.md#implementation) is still relevant.

## Contents

- [Summary](#summary)
- [Data Generation](#data-generation)
- [Data Processing](#data-processing)
    - [Parsing](#parsing)
    - [Chunk Distribution](#chunk-distribution)
- [Training](#training)
    - [Loss](#loss)
- [Results](#results)

## Data Generation

Data was generated by running ```/tuning/generate_data.py```.

~1.2 billion positions were generated from Pingu 3.0.0 self-play games. The dataset can be found [here](https://www.kaggle.com/datasets/williamching/pingu-3-0-0-self-play-data).

Each self-play game consisted of random moves for the first 6 ply, and then a depth 8 search for the remaining moves. Only positions for which the best move was quiet were included in the dataset.

For each position, the evaluation score (relative to white) and the final game result were recorded. The self-play games had a ply limit of 200 ply to avoid over-representation of long drawn endgames, and when a game exceeded this ply limit, the outcome was determined via arbitrary score cut-off (|score| < 400 judged as draw).

## Data Processing

### Parsing

Data was parsed by running ```/tuning/parse_data.py```.

The raw data was mapped as follows:
- Fen -> Sparse array (length 32)
- Eval unchanged
- Result {0, 0.5, 1} -> {0, 1, 2}

The parsed dataset was stored in a numpy memmap of datatype ```np.short```. Mapping the result to ```np.short``` was more space-efficient than storing it as a float.

The sparse array contained 32 integers representing the pieces on the board. For fewer than 32 pieces, some elements were repeated.

```math
\displaystyle x \text{ in sparse} \implies \left\lbrace \begin{array}{rcl} \displaystyle \text{piece} & \displaystyle = & \displaystyle \left\lfloor\frac{x}{64}\right\rfloor \\ \\ \displaystyle \text{square} & \displaystyle = & \displaystyle x\pmod{64} \end{array} \right.
```

Since the raw dataset was stored in multiple files, multiprocessing was used to parse several files at once.

### Chunk Distribution

Data was randomly distributed into chunks by running ```/tuning/split_data.py```, following the method described [here](https://blog.janestreet.com/how-to-shuffle-a-big-dataset/).

The number of chunks was chosen to obtain an expected chunk size of 25 million. Each data sample was then randomly and uniformly assigned to a chunk.

The chunk size was small enough that each chunk could be loaded into RAM. However, it was not too small, otherwise SSD to RAM transfer would become a bottleneck.

## Training

The model was trained by running ```/tuning/nnue_trainer.py```.

### Loss

The main change to the training script was the incorporation of game result into the loss function.

```py
def custom_loss(output, targetEval, targetResult):
    K = 0.00475
    GAMMA = 0.5
    output_scaled = torch.sigmoid(K*output)
    target_scaled = GAMMA * torch.sigmoid(K*targetEval) + (1. - GAMMA) * targetResult
    return torch.mean((output_scaled - target_scaled)**2)
```

It was found that ```GAMMA = 0.5``` was optimal.

| GAMMA | Elo gain vs. Pingu-dev | Number of games |
| ----- | ---------------------- | --------------- |
|  0.5  |        155 ± 23        |       780       |
|  0.2  |        150 ± 21        |       786       |
|  0.8  |        128 ± 20        |       902       |

| GAMMA | Elo gain vs. GAMMA = 0.2 | Number of games |
| ----- | ------------------------ | --------------- |
|  0.5  |  9.39 ± 5.00             |     11,322      |

Pingu-dev represents the latest commit of Pingu before the updated NNUE was introduced. Matches are the results of SPRT testing - 10.0+0.1s, single core, 8 MB hash, LLR > 2.94 [0.00, 3.00].

## Results

The updated NNUE was introduced in [this commit](https://github.com/WillChing01/Pingu/commit/212f4aff02b066053c7ecb8fd9991a258d10de93), leading to gains of ~160 elo at short time controls.
