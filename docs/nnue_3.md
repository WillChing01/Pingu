# NNUE 3

## Summary

This document contains information about Pingu's NNUE. It is the sequel to [nnue_2.md](/docs/nnue_2.md) and it describes the process of training the NNUE for [Pingu 5.0.0](https://github.com/WillChing01/Pingu/releases/tag/v5.0.0).

The transition from Pingu 4.0.0 to Pingu 5.0.0 involved training two successive models of increasing complexity, leading to cumulative gains of ~170 elo at short time controls. In addition to the changes in model architecture, several large improvements were made to the machine learning pipeline, streamlining the process of data generation, preprocessing, and training.

## Contents

- [Summary](#summary)
- [Model Architecture](#model-architecture)
- [Data Generation](#data-generation)
- [Data Processing](#data-processing)

## Model Architecture

Pingu 5.0.0 introduced a Half-KA architecture to the engine's NNUE.

The input-layer consists of two perspective networks (side-to-move and other side). Each perspective network has an input layer consisting of 45,056 binary input features.

```math
\text{stm\_input\_layer}[64 \cdot 11 \cdot 64 \cdot i + 64 \cdot j + k] =
\begin{cases}
1, & \text{if side-to-move's king is on square } i \text{ and piece } j \text{ is on square } k \\
0, & \text{otherwise}
\end{cases}
```

```math
\text{os\_input\_layer}[64 \cdot 11 \cdot 64 \cdot i + 64 \cdot j + k] =
\begin{cases}
1, & \text{if other side's king is on square } i \text{ and piece } j \text{ is on square } k \\
0, & \text{otherwise}
\end{cases}
```

Half-KA gives the network separate weights for each king square. This is useful because evaluation often depends heavily on king position. The input layer is extremely sparse, having a maximum of 31 active features in an input layer of 45046. This allows for fast recomputation when the friendly (from that perspective) king's position changes. In addition, the one-hot encoding allows for fast incremental updates of the first hidden layer when another piece moves.

The architecture of the final model used in Pingu 5.0.0 is shown below. Each layer is fully connected. The initial network was similar except it only had one set of weights before the final output.

<div align="center">
    <img src="img/nnue_architecture.png"/>
</div>

## Data Generation

For the initial Half-KA network 1.19 billion positions were generated by self-play games from the start position, playing either 4 or 6 random ply, then searching for a depth of 8-10 depending on game phase. For the final network, ~3 billion positions were generated from self-play games starting from various opening and endgame positions, searching for 25k nodes per move.

A key improvement from the previous NNUE was the automated uploading of data to [Hugging Face](https://huggingface.co/) repositories. This made it easier to coordinate data generation on multiple machines.

## Data Processing

Scripts for processing data were rewritten to optimise for time and space given the sheer scale of the dataset used to train the final model. Each position was represented as a 40-byte datum. The raw dataset was parsed and randomly divided into training/validation chunks (expected size 25M) with a multithreaded C++ script.

## Training

### Custom Data Loader

### Custom Loss Function

```python
def custom_loss(output, targetEval, targetResult, pieceCounts):
    K = 1 / 400
    GAMMA = 0.8 * (pieceCounts - 2) / 30
    output_scaled = torch.sigmoid(K * output)
    target_scaled = GAMMA * torch.sigmoid(K * targetEval) + (1.0 - GAMMA) * targetResult
    return torch.mean((output_scaled - target_scaled) ** 2)
```
